# Backend Service for AI Mock Interview

## 1. Project Overview

This project is a mock interview platform that helps candidates prepare for real job interviews through an AI-powered interviewer.

A candidate logs in, uploads their resume, and specifies the company & role they are preparing for.

The system parses the resume, stores embeddings in a vector database, and retrieves contextually relevant questions.

The AI interviewer (powered by Cerebras Llama 3.3) conducts the interview in real-time over LiveKit, with Cartesia STT/TTS enabling natural speech-to-speech interaction.

The interview adapts dynamically:

Starts with resume-based questions

Moves to role/company-specific technical or behavioral questions

Adds DSA or system design questions depending on the role context

After the interview, the candidate receives automated feedback generated by the AI, covering their strengths, weaknesses, and improvement areas.

This repository contains the backend for **[AI Mock Interview]**, built with **FastAPI**.  
It handles core functionalities like user authentication, interview management, feedback collection, and centralized configuration.  

It also integrates AI services, live communication, and object storage for resumes.

---

## 2. Architecture
The backend follows a **domain-driven architecture (DDD)**, with self-contained domains:

### **Domains**
1. **Auth** – User registration, login, roles, and JWT-based authentication.
2. **Interview** – Creation, scheduling, and retrieval of interviews.
3. **Feedback** – Collecting and managing feedback from interviewers or candidates.
4. **Config** – Centralized configurations

### **External Integrations**
- **Cerebras & LLaMA 3.3** – For AI-based tasks like question generation or scoring.
- **Cartesia** – Speech-to-text and speech analysis services.
- **LiveKit** – Real-time video/audio communication for interviews.
- **PostgreSQL + pgvector** – Database with vector search for AI embeddings.
- **AWS Object Storage (S3)** – Resume file storage.

---

## 3. Project Structure

```
backend/
│
├── auth/
│ ├── models.py
│ ├── schemas.py
│ ├── services.py
│ ├── routes.py
│ └── init.py
│
├── interview/
│ ├── models.py
│ ├── schemas.py
│ ├── services.py
│ ├── routes.py
│ └── init.py
│
├── feedback/
│ ├── models.py
│ ├── schemas.py
│ ├── services.py
│ ├── routes.py
│ └── init.py
│
├── config/
│ ├── database.py
│ ├── object_storage.py
│ ├── settings.py
│ └── init.py
│
├── main.py
├── requirements.txt
├── alembic/
├── tests/
├── Dockerfile
└── .github/workflows/ci-cd.yml
```

---

## 4. Tech Stack
- **Backend Framework:** FastAPI
- **Language:** Python 3.10+
- **Database:** PostgreSQL + pgvector
- **Object Storage:** AWS S3
- **Authentication:** JWT tokens
- **Real-time Communication:** LiveKit
- **AI Services:** Cerebras, LLaMA 3.3
- **Speech Processing:** Cartesia
- **Containerization:** Docker
- **Deployment:** AWS
- **CI/CD:** GitHub Actions

---

## 5. Key Features

- Hybrid Interview Flow (STT → Batch LLM → Streaming TTS)

    - Candidate speaks naturally.

    - Pause detection triggers AI response.

    - AI replies in streaming voice for a realistic, fluid conversation.

- Resume-Aware Questioning

    - Candidate uploads a resume.

    - Resume parsed, embedded, and stored in pgvector.

    - AI interviewer asks contextual questions about past experience & projects.

- Company & Role-Specific Questioning

    - Web-scraped / pre-curated Q&A pairs stored in vector DB.

    - Relevant interview questions retrieved dynamically based on chosen role/company.

- Adaptive Question Flow

    - Interview adapts in real-time.

    - Starts with resume-based intro → transitions to role-specific technical/behavioral → adds DSA/system design as needed.

- Structured Feedback Generation

    - AI provides post-interview feedback on clarity, depth, technical correctness, and communication.

    - Stored in Postgres for candidate review.

- Monolithic Backend (MVP Ready)

    - All services (Auth, Resume, Interview, Feedback) run inside one FastAPI app.

    - Easier to iterate, debug, and deploy.

- Vector Database Integration (pgvector + Postgres)

    - Used for resume embeddings, question bank, and ideal answer comparisons.

    - Enables semantic search and contextual question generation.

---

## 6. Development Setup

### Prerequisites
- Python 3.10+
- Docker and Docker Compose
- Git

### Quick Start

#### 1. Clone and Setup Environment
```bash
# Clone the repository
git clone <repository-url>
cd ai-mock-interview/backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### 2. Database Setup with Docker

**Start PostgreSQL with pgvector:**
```bash
# Start PostgreSQL with pgvector extension
docker run --name ai-mock-interview-db \
  -e POSTGRES_DB=ai_mock_interview \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=password \
  -p 5432:5432 \
  -d pgvector/pgvector:pg16

# Verify database is running
docker ps
```

**Alternative: Using Docker Compose (Recommended)**
```bash
# Create docker-compose.yml
cat > docker-compose.yml << EOF
version: '3.8'
services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: ai-mock-interview-db
    environment:
      POSTGRES_DB: ai_mock_interview
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
EOF

# Start the database
docker-compose up -d

# Check if database is healthy
docker-compose ps
```

#### 3. Environment Configuration

**Create .env file:**
```bash
# Copy the sample environment file
cp .env.sample .env

# Edit the .env file with your settings
nano .env
```

**Required .env variables:**
```env
# Database Configuration
DATABASE_URL=postgresql://postgres:password@localhost:5432/ai_mock_interview

# JWT Configuration
JWT_SECRET_KEY=your-secret-key-change-this-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=180

# AWS Configuration (for S3)
AWS_ACCESS_KEY_ID=your-aws-access-key
AWS_SECRET_ACCESS_KEY=your-aws-secret-key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-s3-bucket-name

# AI Services
CEREBRAS_API_KEY=your-cerebras-api-key
CARTESIA_API_KEY=your-cartesia-api-key

# LiveKit Configuration
LIVEKIT_API_KEY=your-livekit-api-key
LIVEKIT_API_SECRET=your-livekit-api-secret
LIVEKIT_URL=wss://your-livekit-url
```

#### 4. Database Migration (Optional)

**For production or when you want to use Alembic migrations:**
```bash
# Run database migrations
alembic upgrade head
```

**Note:** The application will automatically create tables on startup if migrations haven't been run.

#### 5. ERD Diagram

```mermaid
erDiagram
    USERS {
        UUID user_id PK
        VARCHAR username
        VARCHAR email
        TEXT password
        TIMESTAMP created_at
        TIMESTAMP updated_at
    }

    RESUMES {
        UUID id PK
        UUID user_id FK
        TEXT file_name
        TEXT file_url
        VECTOR(1536) embedding_vector
        TIMESTAMP uploaded_at
    }

    INTERVIEWS {
        UUID id PK
        UUID candidate_id FK
        VARCHAR status
        INTERVAL duration
        TIMESTAMP created_at
        TIMESTAMP updated_at
    }

    INTERVIEW_TRANSCRIPTS {
        UUID id PK
        UUID interview_id FK
        VARCHAR speaker
        TEXT message_text
        TIMESTAMP created_at
    }

    FEEDBACK {
        UUID id PK
        UUID interview_id FK
        INTEGER rating
        JSONB scores
        TEXT comments
        TIMESTAMP created_at
        TIMESTAMP updated_at
    }

    USERS ||--o{ RESUMES : uploads
    USERS ||--o{ INTERVIEWS : "candidate"
    INTERVIEWS ||--o{ INTERVIEW_TRANSCRIPTS : has
    INTERVIEWS ||--o{ FEEDBACK : has

```

#### 6. Start the Application

```bash
# Start the FastAPI server
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Or using Python directly
python -m uvicorn main:app --reload
```

#### 6. Verify Setup

**Check if everything is working:**
```bash
# Health check
curl http://localhost:8000/health

# API documentation
open http://localhost:8000/docs
```

### Development Commands

**Database Management:**
```bash
# Connect to database
docker exec -it ai-mock-interview-db psql -U postgres -d ai_mock_interview

# Stop database
docker stop ai-mock-interview-db

# Start database
docker start ai-mock-interview-db

# Remove database (WARNING: This deletes all data)
docker rm ai-mock-interview-db
```

**Alembic Commands:**
```bash
# Create new migration
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Rollback migration
alembic downgrade -1

# Check migration status
alembic current
```

**Application Commands:**
```bash
# Install dependencies
pip install -r requirements.txt

# Run with auto-reload
uvicorn main:app --reload

# Run tests (when implemented)
pytest

# Code formatting
black .
isort .
```

### Troubleshooting

**Database Connection Issues:**
```bash
# Check if PostgreSQL is running
docker ps | grep postgres

# Check database logs
docker logs ai-mock-interview-db

# Test connection
docker exec -it ai-mock-interview-db pg_isready -U postgres
```

**Port Conflicts:**
```bash
# Check if port 5432 is in use
lsof -i :5432

# Use different port
docker run -p 5433:5432 pgvector/pgvector:pg16
```

**Environment Issues:**
```bash
# Check Python version
python --version

# Recreate virtual environment
rm -rf venv
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
